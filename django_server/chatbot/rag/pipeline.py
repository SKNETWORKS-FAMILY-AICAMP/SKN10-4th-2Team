import requests
import re
from .classification import classify_category
from .multiquery import generate_multi_queries
from .relevance_check import is_relevant
from .internal_rag import search_documents, generate_answer_with_docs
from .external_rag import tavily_search

def strip_badge(text: str) -> str:
    """
    ì´ë¯¸ ë¶™ì€ badgeë¥¼ ì œê±°í•˜ëŠ” í›„ì²˜ë¦¬ í•¨ìˆ˜ (ì¤‘ë³µ ì œê±° ë°©ì§€)
    """
    return re.sub(r"<br\s*/?><br\s*/?><span class=['\"]?badge['\"]?>.*?</span>", "", text, flags=re.IGNORECASE)

def sllm_answer(prompt: str, category: str = None, history: list = None) -> str:
    """
    ë¡œì»¬ Ollamaì—ì„œ gemma3-wine ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ REST APIë¡œ ì‘ë‹µ ìƒì„±.
    """
    system_prompt = (
        """
        ë‹¹ì‹ ì€ ì™€ì¸ ë¶„ì•¼ì— íŠ¹í™”ëœ ì „ë¬¸ì ì¸ AI íë ˆì´í„°ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

        - ë°˜ë“œì‹œ "HTML í˜•ì‹"ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
        - í•µì‹¬ ìš”ì•½ â†’ ëª©ë¡(`<ul><li>`) â†’ ë³¸ë¬¸ ì„¤ëª… ìˆœìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
        - ì¤„ë°”ê¿ˆ(`<br>`)ê³¼ ê°•ì¡°(`<b>`, `<i>`) íƒœê·¸ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
        - ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ ì¶”ë ¤ì„œ ì‚¬ìš©í•˜ì„¸ìš”.
        - ë‹µì„ ëª¨ë¥¼ ê²½ìš° "ì •í™•í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”.
        - "ì €ëŠ” AIì…ë‹ˆë‹¤"ë¼ëŠ” ë¬¸êµ¬ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        ì˜ˆì‹œ í˜•ì‹:
        ìš”ì•½ ë¬¸ì¥<br>
        <ul>
        <li>í•µì‹¬ í¬ì¸íŠ¸ 1</li>
        <li>í•µì‹¬ í¬ì¸íŠ¸ 2</li>
        </ul>
        <br><br>ë³¸ë¬¸ ì„¤ëª… ë¬¸ë‹¨<br><br>

        ì§ˆë¬¸: {question}
        """
    )

    full_prompt = f"{system_prompt}\n\n{prompt}"

    try:
        response = requests.post(
            "http://localhost:11434/api/generate",  # Ollama REST API ì—”ë“œí¬ì¸íŠ¸
            json={
                "model": "gemma3-wine",  # ì •í™•í•œ ëª¨ë¸ ì´ë¦„
                "prompt": full_prompt,
                "stream": False
            },
            timeout=60  # ì´ˆ ë‹¨ìœ„ íƒ€ì„ì•„ì›ƒ
        )

        if response.status_code == 200:
            return response.json().get("response", "")
        else:
            return f"[ì˜¤ë¥˜] Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {response.text}"

    except requests.exceptions.RequestException as e:
        return f"[ì˜¤ë¥˜] Ollama API ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}"

def sllm_greeting_answer(prompt: str, category: str = None, history: list = None) -> str:
    """
    ë¡œì»¬ Ollamaì—ì„œ gemma3-wine ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ REST APIë¡œ ì‘ë‹µ ìƒì„±.
    """
    system_prompt = (
        """
        ë‹¹ì‹ ì€ ì™€ì¸ ë¶„ì•¼ì— íŠ¹í™”ëœ ì „ë¬¸ì ì¸ AI íë ˆì´í„°ì…ë‹ˆë‹¤.
        ì‚¬ìš©ì ì¸ì‚¬ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

        - ë°˜ë“œì‹œ "HTML íƒœê·¸ë¥¼ í¬í•¨í•œ ì‹¤ì œ HTML í˜•ì‹"ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
        - ì ˆëŒ€ ```html ë˜ëŠ” ì½”ë“œë¸”ë¡ì²˜ëŸ¼ ê°ì‹¸ì§€ ë§ˆì„¸ìš”.
        - <br>, <b>, <i> ë“±ì˜ HTML íƒœê·¸ë¥¼ ììœ ë¡­ê²Œ ì‚¬ìš©í•´ ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ì§ˆë¬¸: {question}
        """
    )

    full_prompt = f"{system_prompt}\n\n{prompt}"

    try:
        response = requests.post(
            "http://localhost:11434/api/generate",  # Ollama REST API ì—”ë“œí¬ì¸íŠ¸
            json={
                "model": "gemma3-wine",  # ì •í™•í•œ ëª¨ë¸ ì´ë¦„
                "prompt": full_prompt,
                "stream": False
            },
            timeout=60  # ì´ˆ ë‹¨ìœ„ íƒ€ì„ì•„ì›ƒ
        )

        if response.status_code == 200:
            return response.json().get("response", "")
        else:
            return f"[ì˜¤ë¥˜] Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {response.text}"

    except requests.exceptions.RequestException as e:
        return f"[ì˜¤ë¥˜] Ollama API ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}"
    
def get_final_answer(user_question: str, history: list) -> str:
    """
    ì „ì²´ ì§ˆë¬¸ íë¦„ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜.
    ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ë¶„ê¸°í•˜ë©°,
    ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰, LLM ì„œë¸Œì§ˆë¬¸, ì™¸ë¶€ ê²€ìƒ‰ ë“±ì„ ë‹¨ê³„ì ìœ¼ë¡œ í™œìš©.
    """
    print(f"[ğŸ’¬] ì§ˆë¬¸: {user_question}")
    category = classify_category(user_question)
    print(f"[ğŸ’¬] ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")

    answer = ""
    badge = ""

    if category == "greeting":
        answer = sllm_greeting_answer(user_question, category, history)
        badge = "ğŸ§  LLM"

    elif category == "etc":
        temp = sllm_answer(user_question, category, history)
        if is_relevant(user_question, temp):
            answer, badge = temp, "ğŸ§  LLM"
        else:
            docs = search_documents("wine", user_question)
            if docs:
                temp = generate_answer_with_docs(user_question, docs)
                if is_relevant(user_question, temp):
                    answer, badge = temp, "ğŸ“ ë‚´ë¶€ ë¬¸ì„œ"
        if not answer:
            answer = tavily_search(user_question)
            badge = "ğŸŒ ì™¸ë¶€ ë¬¸ì„œ"

    else:  # wine, grape, region, producer
        sub_questions = generate_multi_queries(user_question)
        print(f"[ğŸ’¬] ìƒì„±ëœ ì„œë¸Œ ì§ˆë¬¸ë“¤: {sub_questions}")

        docs = search_documents(category, user_question)
        if docs:
            temp = generate_answer_with_docs(user_question, docs)
            if is_relevant(user_question, temp):
                answer, badge = temp, "ğŸ“ ë‚´ë¶€ ë¬¸ì„œ"

        if not answer:
            for sub_q in sub_questions:
                temp = sllm_answer(sub_q, category, history)
                print(f"[ğŸ’¬] '{sub_q}' â†’ ì‘ë‹µ: {temp[:60]}...")
                if is_relevant(user_question, temp):
                    answer, badge = temp, "ğŸ§  LLM"
                    break

        if not answer:
            answer = tavily_search(user_question)
            badge = "ğŸŒ ì™¸ë¶€ ë¬¸ì„œ"

    cleaned = strip_badge(answer.strip())
    return cleaned + f"<br><br><span class='badge'>{badge}</span>"
